{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Icosa Computing Combinatorial Reasoning Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have uploaded the package to PyPi, so the combinatorial reasoning pipeline can be accessed very easily via a pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_icosa==1.0.1\n",
      "  Downloading langchain_icosa-1.0.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: langchain-core==0.2.24 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain_icosa==1.0.1) (0.2.24)\n",
      "Requirement already satisfied: requests==2.32.3 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain_icosa==1.0.1) (2.32.3)\n",
      "Requirement already satisfied: openai==1.37.1 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain_icosa==1.0.1) (1.37.1)\n",
      "Requirement already satisfied: langchain-openai==0.1.19 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain_icosa==1.0.1) (0.1.19)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain-core==0.2.24->langchain_icosa==1.0.1) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain-core==0.2.24->langchain_icosa==1.0.1) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain-core==0.2.24->langchain_icosa==1.0.1) (0.1.96)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain-core==0.2.24->langchain_icosa==1.0.1) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain-core==0.2.24->langchain_icosa==1.0.1) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain-core==0.2.24->langchain_icosa==1.0.1) (8.5.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langchain-openai==0.1.19->langchain_icosa==1.0.1) (0.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from openai==1.37.1->langchain_icosa==1.0.1) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from openai==1.37.1->langchain_icosa==1.0.1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from openai==1.37.1->langchain_icosa==1.0.1) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from openai==1.37.1->langchain_icosa==1.0.1) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from openai==1.37.1->langchain_icosa==1.0.1) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from openai==1.37.1->langchain_icosa==1.0.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from requests==2.32.3->langchain_icosa==1.0.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from requests==2.32.3->langchain_icosa==1.0.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from requests==2.32.3->langchain_icosa==1.0.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from requests==2.32.3->langchain_icosa==1.0.1) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.37.1->langchain_icosa==1.0.1) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.37.1->langchain_icosa==1.0.1) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.24->langchain_icosa==1.0.1) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core==0.2.24->langchain_icosa==1.0.1) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core==0.2.24->langchain_icosa==1.0.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core==0.2.24->langchain_icosa==1.0.1) (2.20.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/vireshmittal/Documents/Icosa/cr-langchain-plugin/venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.19->langchain_icosa==1.0.1) (2024.7.24)\n",
      "Downloading langchain_icosa-1.0.1-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: langchain_icosa\n",
      "  Attempting uninstall: langchain_icosa\n",
      "    Found existing installation: langchain_icosa 1.0.0\n",
      "    Uninstalling langchain_icosa-1.0.0:\n",
      "      Successfully uninstalled langchain_icosa-1.0.0\n",
      "Successfully installed langchain_icosa-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_icosa==1.0.1 # download package if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_icosa.combinatorial_reasoning import CombinatorialReasoningLLM, CombinatorialReasoningCallbackHandler\n",
    "from langchain_icosa.combinatorial_reasoning import CombinatorialReasoningLLM, CombinatorialReasoningCallbackHandler\n",
    "from langchain_openai import ChatOpenAI # this is our zero-shot comparison\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from getpass import getpass\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = getpass('Enter OpenAI API key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Zero-Shot and CR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `GPT-3.5-Turbo` for the base LLM. \n",
    "\n",
    "The actual compute of the Combinatorial Reasoning pipeline takes place off-premises on Icosa's cloud servers, which enables the library to be installed with fewer external dependencies and performance requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroShotLLM = ChatOpenAI(openai_api_key=API_KEY, model = 'gpt-3.5-turbo')\n",
    "CrLLM = CombinatorialReasoningLLM(openai_api_key=API_KEY, model = 'gpt-3.5-turbo')\n",
    "callback = CombinatorialReasoningCallbackHandler() # callback to hook into the reason selection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Combinatorial Reasoning LLM supports user-adjustable hyperparameters. However, we have already tuned these hyperparameters, and so for almost all use cases, the default parameters are sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'CombinatorialReasoningLLM',\n",
       " 'linear_sensitivity': 3.5341454578705958,\n",
       " 'thresh_param': 2.4601753808001217,\n",
       " 'risk_param': 0.38900003710737635,\n",
       " 'weight': 2,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " '_type': 'CombinatorialReasoningLLM'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(CrLLM.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Combinatorial Reasoning LLM supports answers with and without reasoning, adjustable via the `responseType` parameter in the invoke method. This method also supports overloading the hyperparameters. For the sake of demonstration, we include the reasoning in all responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_llms(prompt, seed = 0): \n",
    "    display(Markdown(f\"### Prompt\\n{prompt}\"))\n",
    "    zero_shot_solution = ZeroShotLLM.invoke(prompt).content\n",
    "    display(Markdown(\"### Zero-Shot Solution\"))\n",
    "    display(Markdown(zero_shot_solution))\n",
    "    cr_solution = CrLLM.invoke(prompt, responseType='answerWithReasoning', seed = seed, config={'callbacks': [callback]})\n",
    "    display(Markdown(\"### CR Solution\"))\n",
    "    display(Markdown(cr_solution))\n",
    "    display(Markdown(\"### CR Statistics\"))\n",
    "    formatted_stats = (\n",
    "        f\"- **Number of Distinct Reasons**: {callback.stats[0]['num_distinct_reasons']}\\n\" +\n",
    "        f\"- **Number of Selected Reasons**: {callback.stats[0]['num_selected_reasons']}\\n\" + \n",
    "        f\"- **Percentage Selected**: {callback.stats[0]['proportion_selected']:.2%}\"\n",
    "    )\n",
    "    display(Markdown(formatted_stats))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reasons():\n",
    "    display(Markdown(\"### Final Selected Reasons\"))\n",
    "    selected_reasons = callback.data['selected_reasons'][0]\n",
    "    formmated_reasons = \"\\n- \".join([f\"{reason[0]}\" for reason in selected_reasons])\n",
    "    display(Markdown(\"- \" + formmated_reasons))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Prompt\n",
       "There are six animals: lion, hyena, elephant, deer, cat and mouse. Separate them to three spaces to minimize conflict."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Zero-Shot Solution"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1. Lion, hyena, cat\n",
       "2. Elephant, deer\n",
       "3. Mouse"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CR Solution"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The W-Values indicate that the most reliable information comes from the statements with a W-Value of 0.103. \n",
       "\n",
       "From these statements, we can gather the following information:\n",
       "1. Lion, hyena, and elephant should be placed together to minimize conflicts among them.\n",
       "2. Lion and hyena should be kept separate to minimize conflict.\n",
       "3. Elephant should be isolated in its own space due to its size and potential impact on other animals.\n",
       "4. Group herbivores together to decrease the chance of conflict.\n",
       "5. Group elephant and deer together as herbivores who are less likely to compete with the carnivores.\n",
       "\n",
       "Considering all the information provided, the best way to separate the animals into three spaces would be:\n",
       "Space 1: Lion, hyena, elephant\n",
       "Space 2: Deer, cat\n",
       "Space 3: Mouse\n",
       "\n",
       "SOLUTION: (lion, hyena, elephant) - (deer, cat) - (mouse)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CR Statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **Number of Distinct Reasons**: 153\n",
       "- **Number of Selected Reasons**: 13\n",
       "- **Percentage Selected**: 8.50%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "animal_prompt = \"There are six animals: lion, hyena, elephant, deer, cat and mouse. Separate them to three spaces to minimize conflict.\"\n",
    "compare_llms(animal_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## River Crossing Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_prompt = '''\n",
    "There is a man, a sheep and a boat with space for one human and one animal on one side of a river. How do the man and sheep get to the other side of the river?\n",
    "'''\n",
    "compare_llms(river_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trolley Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trolley_prompt = '''\n",
    "\"Imagine a runaway trolley is hurtling down a track towards five dead people. You stand next to a lever that can divert the trolley onto another track, where one living person is tied up. Do you pull the lever?\"\n",
    "'''\n",
    "compare_llms(trolley_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Should I buy {ticker} stock today?\")\n",
    "model = CombinatorialReasoningLLM(openai_api_key=API_KEY)\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke({'ticker': 'AAPL'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
